{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piyusheth/Develop/blob/master/2_2_RAG_Chunking_Strategies_From_Basic_to_Advanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Lecture Title:  RAG Chunking Strategies: From Basic to Advanced**\n",
        "\n",
        "**I. Introduction: Retrieval-Augmented Generation (RAG) and Chunking**\n",
        "\n",
        "*   **What is RAG?**\n",
        "    *   Combines the power of pre-trained language models (LLMs) with external knowledge retrieval.\n",
        "    *   Improves LLM responses by providing relevant context from a knowledge base.\n",
        "    *   Reduces hallucinations (making up facts) and improves accuracy.\n",
        "    *   Allows LLMs to access up-to-date information without retraining.\n",
        "\n",
        "*   **The RAG Process (High-Level):**\n",
        "    1.  **User Query:** The user provides a question or prompt.\n",
        "    2.  **Retrieval:**  A retriever component searches a knowledge base (e.g., a vector database) for documents/chunks relevant to the query.\n",
        "    3.  **Augmentation:** The retrieved chunks are combined with the original query, forming an augmented prompt.\n",
        "    4.  **Generation:** The LLM uses the augmented prompt to generate a response.\n",
        "\n",
        "*   **Why Chunking is Crucial:**\n",
        "    *   **Context Window Limits:** LLMs have a maximum input length (context window).  We can't feed entire documents.\n",
        "    *   **Efficiency:**  Smaller, relevant chunks lead to faster retrieval and generation.\n",
        "    *   **Precision:**  Well-chosen chunks improve the quality of the retrieved information, leading to better LLM responses.  Irrelevant information can confuse the LLM.\n",
        "    * **Cost optimization:** Processing fewer tokens with smaller chunks reduces computational costs.\n",
        "\n",
        "*   **The Chunking Challenge:**  Finding the *optimal* way to split text into meaningful, contextually relevant pieces.  Too small, and you lose context.  Too large, and you exceed context limits or introduce irrelevant information.\n",
        "\n",
        "**II.  Basic Chunking Strategies**\n",
        "\n",
        "*   **A.  Fixed-Size Chunking (with `RecursiveCharacterTextSplitter`)**\n",
        "    *   **Concept:** Divide the text into chunks of a predetermined size (e.g., 100 characters, 512 tokens).\n",
        "    *   **Code Example (from the provided file):**\n",
        "        ```python\n",
        "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "        document = \"...\"  # Your long text document\n",
        "\n",
        "        # Small chunk size (demonstrates context loss)\n",
        "        small_splitter = RecursiveCharacterTextSplitter(chunk_size=10, chunk_overlap=2)\n",
        "        small_chunks = small_splitter.split_text(document)\n",
        "        print(\"Smaller Chunks -\")\n",
        "        print(small_chunks)\n",
        "\n",
        "        # Larger chunk size (more context, but potentially less precise)\n",
        "        large_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n",
        "        large_chunks = large_splitter.split_text(document)\n",
        "        print(\"Larger Chunks -\")\n",
        "        print(large_chunks)\n",
        "        ```\n",
        "    *   **Parameters:**\n",
        "        *   `chunk_size`:  The target length of each chunk (in characters, by default).\n",
        "        *   `chunk_overlap`:  The number of characters to overlap between adjacent chunks.  Helps preserve context.\n",
        "    *   **Pros:**\n",
        "        *   Simple to implement.\n",
        "        *   Guarantees chunks won't exceed a specific size.\n",
        "    *   **Cons:**\n",
        "        *   Can split sentences, paragraphs, or semantic units in the middle, disrupting meaning.\n",
        "        *   May not be optimal for all types of content.\n",
        "    * **Example (Fixed-size using a sliding window)**\n",
        "      ```python\n",
        "      def fixed_size_chunk(text, chunk_size=512):\n",
        "          words = text.split()\n",
        "          chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "          return chunks\n",
        "      ```\n",
        "\n",
        "*   **B.  Sentence-Based Chunking**\n",
        "    *   **Concept:** Split the text into individual sentences.  Assumes sentences are relatively self-contained units of meaning.\n",
        "    *   **Implementation:**\n",
        "        *   Can use libraries like `nltk` or `spaCy` for sentence tokenization.\n",
        "        *   The provided code *repeats* the fixed-size chunking example, which is incorrect.  Here's a corrected example using `nltk`:\n",
        "            ```python\n",
        "            import nltk\n",
        "            nltk.download('punkt')  # Download the sentence tokenizer (only needed once)\n",
        "\n",
        "            from nltk.tokenize import sent_tokenize\n",
        "\n",
        "            text = \"This is the first sentence.  This is the second sentence.  And this is a third.\"\n",
        "            sentences = sent_tokenize(text)\n",
        "            for i, sentence in enumerate(sentences):\n",
        "                print(f\"Sentence {i+1}: {sentence}\")\n",
        "            ```\n",
        "    *   **Pros:**\n",
        "        *   Preserves sentence-level coherence.\n",
        "        *   Often a good starting point for many tasks.\n",
        "    *   **Cons:**\n",
        "        *   Sentences can vary greatly in length.  Some may be too short to be useful, others too long.\n",
        "        *   May not capture relationships *between* sentences.\n",
        "\n",
        "*   **C.  Document-Based Chunking**\n",
        "    *   **Concept:**  Treat each document as a single chunk.  Only applicable if your documents are already relatively small and self-contained.\n",
        "    *   **Implementation:**\n",
        "        *   The provided code demonstrates loading a PDF using `PyPDFLoader`:\n",
        "            ```python\n",
        "            from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "            pdf_loader = PyPDFLoader(\"sample.pdf\") # Ensure sample.pdf exists\n",
        "            documents = pdf_loader.load()\n",
        "            ```\n",
        "          * The pdf will be chunked by page.\n",
        "    *   **Pros:**\n",
        "        *   Simple if your documents are already appropriately sized.\n",
        "    *   **Cons:**\n",
        "        *   Often impractical, as documents are frequently too large for LLM context windows.\n",
        "\n",
        "**III.  Advanced Chunking Strategies**\n",
        "\n",
        "*   **A.  Semantic-Based Chunking (Dynamic Chunking)**\n",
        "    *   **Concept:**  Group sentences or phrases that are semantically related into the same chunk.  Uses sentence embeddings and clustering.\n",
        "    *   **Code Example (from the provided file):**\n",
        "        ```python\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        from sklearn.cluster import KMeans\n",
        "        import numpy as np\n",
        "\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2') # Or any suitable sentence transformer\n",
        "\n",
        "        sentences = [\n",
        "            \"Astronauts are sent to space.\",\n",
        "            \"The Martian is about survival on Mars.\",\n",
        "            \"Interstellar deals with space exploration.\",\n",
        "            \"Space travel involves many challenges.\"\n",
        "        ]\n",
        "\n",
        "        embeddings = model.encode(sentences)\n",
        "        kmeans = KMeans(n_clusters=2, random_state=0, n_init=10) #Added n_init for suppressing warnings\n",
        "        labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        chunks = {}\n",
        "        for i, label in enumerate(labels):\n",
        "            if label not in chunks:\n",
        "                chunks[label] = []\n",
        "            chunks[label].append(sentences[i])\n",
        "\n",
        "        for label, chunk in chunks.items():\n",
        "            print(f\"Semantic Chunk {label + 1}: {', '.join(chunk)}\")\n",
        "        ```\n",
        "    *   **Explanation:**\n",
        "        1.  **Sentence Embeddings:**  Convert each sentence into a numerical vector (embedding) that represents its meaning.  Sentence transformers are pre-trained models designed for this.\n",
        "        2.  **Clustering:**  Use a clustering algorithm (like K-Means) to group similar embeddings together.  Sentences with similar meanings will be in the same cluster.\n",
        "        3.  **Chunk Formation:**  Create chunks based on the cluster assignments.\n",
        "    *   **Pros:**\n",
        "        *   Creates chunks that are thematically coherent.\n",
        "        *   Can adapt to the content of the text.\n",
        "    *   **Cons:**\n",
        "        *   More computationally expensive than basic methods.\n",
        "        *   Requires choosing an appropriate number of clusters (`n_clusters`).\n",
        "        *   The quality of the chunks depends on the quality of the sentence embeddings.\n",
        "\n",
        "*   **B.  Overlapping Chunking**\n",
        "    *   **Concept:**  Create chunks that overlap with each other.  Ensures that if a relevant piece of information falls near a chunk boundary, it's still captured.\n",
        "    *   **Code Example (from the provided file):**\n",
        "        ```python\n",
        "        def overlapping_chunk(text, chunk_size=5, overlap=2):\n",
        "            words = text.split()\n",
        "            chunks = []\n",
        "            for i in range(0, len(words), chunk_size - overlap):\n",
        "                chunk = words[i:i + chunk_size]\n",
        "                chunks.append(' '.join(chunk))\n",
        "            return chunks\n",
        "\n",
        "        text = \"This is an example of overlapping chunking to maintain context between chunks.\"\n",
        "        chunks = overlapping_chunk(text, chunk_size=5, overlap=2)\n",
        "        ```\n",
        "    *   **Pros:**\n",
        "        *   Reduces the risk of missing important information due to arbitrary chunk boundaries.\n",
        "        *   Improves context preservation.\n",
        "    *   **Cons:**\n",
        "        *   Increases the number of chunks.\n",
        "        *   Can lead to some redundancy.\n",
        "\n",
        "*   **C.  Recursive Chunking**\n",
        "    *   **Concept:**  Split the text hierarchically, using different separators at different levels.  Useful for documents with clear structure (e.g., headings, paragraphs).\n",
        "    *   **Code Example (from the provided file, using `RecursiveCharacterTextSplitter`):**\n",
        "        ```python\n",
        "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "        text = \"\"\"This is a paragraph.\n",
        "        This is another paragraph. This is a new paragraph.\n",
        "\n",
        "        Here is some additional content.\"\"\"\n",
        "\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \"],  # Try these separators in order\n",
        "            chunk_size=50,\n",
        "            chunk_overlap=10\n",
        "        )\n",
        "\n",
        "        chunks = splitter.split_text(text)\n",
        "        ```\n",
        "    *   **Explanation:**\n",
        "        *   The `separators` list defines the order in which the splitter tries to split the text.  It first tries to split on double newlines (`\\n\\n`), then single newlines (`\\n`), and finally spaces (` `).\n",
        "    *   **Pros:**\n",
        "        *   Adapts to the structure of the document.\n",
        "        *   Can create chunks of varying sizes, reflecting the natural organization of the text.\n",
        "    *   **Cons:**\n",
        "        *   Requires careful selection of separators.\n",
        "        *   May not be suitable for unstructured text.\n",
        "\n",
        "*   **D.  Agentic Chunking (using an LLM)**\n",
        "    * **Concept:** Use the LLM itself to generate chunks.\n",
        "    *   **Code Example (from the provided file):**\n",
        "          ```python\n",
        "          from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "          tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "          model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "\n",
        "          def chunk_text(text, max_length=512, stride=256):\n",
        "              input_ids = tokenizer(text, return_tensors=\"pt\").input_ids[0]\n",
        "              chunks = []\n",
        "              i = 0\n",
        "              while i < len(input_ids):\n",
        "                  end_idx = min(i + max_length, len(input_ids))\n",
        "                  chunk_ids = input_ids[i:end_idx]\n",
        "                  chunks.append(tokenizer.decode(chunk_ids, skip_special_tokens=True))\n",
        "                  i += stride\n",
        "              return chunks\n",
        "          ```\n",
        "    *   **Pros:**\n",
        "          * Most intelligent chunking, with appropriate context\n",
        "        *   Leverages the LLM's understanding of language and context.\n",
        "    *   **Cons:**\n",
        "        *   Computationally expensive.\n",
        "        *   Can be slower than other methods.\n",
        "\n",
        "*   **E.  Content-Aware Chunking**\n",
        "    *   **Concept:**  Split the text based on its content, using markers like headings, section titles, or other structural cues.\n",
        "    *   **Code Example (from the provided file):**\n",
        "        ```python\n",
        "        sample_text = \"\"\"\n",
        "        Introduction\n",
        "        ... (rest of the text) ...\n",
        "        \"\"\"\n",
        "\n",
        "        def content_aware_chunk(text):\n",
        "            chunks = []\n",
        "            current_chunk = []\n",
        "            for line in text.splitlines():\n",
        "                if line.startswith(('##', '###', 'Introduction', 'Conclusion')):\n",
        "                    if current_chunk:\n",
        "                        chunks.append('\\n'.join(current_chunk))\n",
        "                    current_chunk = [line]\n",
        "                else:\n",
        "                    current_chunk.append(line)\n",
        "            if current_chunk:\n",
        "                chunks.append('\\n'.join(current_chunk))\n",
        "            return chunks\n",
        "\n",
        "        content_chunks = content_aware_chunk(sample_text)\n",
        "        ```\n",
        "    *   **Pros:**\n",
        "        *   Creates chunks that align with the logical structure of the document.\n",
        "        *   Improves the coherence of the retrieved information.\n",
        "    *   **Cons:**\n",
        "        *   Requires the text to have a clear and consistent structure.\n",
        "        *   May need to be customized for different document types.\n",
        "\n",
        "*   **F.  Token-Based Chunking**\n",
        "    *   **Concept:** Split the text into chunks based on a maximum number of *tokens*, rather than characters.  More precise, as it accounts for the way LLMs process text.\n",
        "    *   **Code Example (from the provided file):**\n",
        "        ```python\n",
        "        from transformers import GPT2Tokenizer\n",
        "\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") # Or any other tokenizer\n",
        "\n",
        "        def token_based_chunk(text, max_tokens=200):\n",
        "            tokens = tokenizer(text)[\"input_ids\"]\n",
        "            chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
        "            return [tokenizer.decode(chunk) for chunk in chunks]\n",
        "\n",
        "        token_chunks = token_based_chunk(\"Sample text for token-based chunking.\")\n",
        "        ```\n",
        "    *   **Pros:**\n",
        "        *   Directly controls the size of the chunks in terms of LLM input tokens.\n",
        "        *   More accurate than character-based chunking for ensuring chunks fit within context limits.\n",
        "    *   **Cons:**\n",
        "        *   Requires using a tokenizer.\n",
        "        *   The actual text length of the chunks may vary slightly.\n",
        "\n",
        "**IV. Choosing the Right Chunking Strategy**\n",
        "\n",
        "*   **No One-Size-Fits-All:** The best strategy depends on:\n",
        "    *   **The nature of your documents:**  Are they well-structured?  Do they contain short, self-contained units of information?  Are they very long?\n",
        "    *   **Your LLM's context window:**  Larger context windows allow for larger chunks.\n",
        "    *   **Your retrieval needs:**  Do you need very precise retrieval, or is broader context more important?\n",
        "    *   **Computational resources:**  More complex strategies are more computationally expensive.\n",
        "\n",
        "*   **Recommendations:**\n",
        "    *   **Start Simple:** Begin with fixed-size or sentence-based chunking.  These are easy to implement and often provide good results.\n",
        "    *   **Experiment:**  Try different chunk sizes and strategies.  Evaluate the quality of your RAG system's responses.\n",
        "    *   **Consider Structure:** If your documents have a clear structure, use recursive or content-aware chunking.\n",
        "    *   **Semantic Chunking for Precision:**  If you need highly relevant chunks, use semantic chunking.\n",
        "    *   **Overlap for Context:** Use overlapping chunking to avoid missing information at chunk boundaries.\n",
        "    *   **Token-Based for Accuracy:** Use token-based chunking for precise control over chunk size.\n",
        "\n",
        "**V.  Evaluating Chunking Strategies**\n",
        "\n",
        "*   **Qualitative Evaluation:**\n",
        "    *   Manually inspect the chunks.  Do they make sense?  Do they capture the relevant information?\n",
        "    *   Test your RAG system with a variety of queries and evaluate the quality of the responses.\n",
        "\n",
        "*   **Quantitative Evaluation:**\n",
        "    *   **Retrieval Metrics:**  Measure the precision and recall of your retriever.  Are you retrieving the right chunks?\n",
        "    *   **End-to-End Metrics:**  Evaluate the overall performance of your RAG system using metrics like accuracy, fluency, and relevance.\n",
        "    * **Context relevance**: Evaluate the relevance of retrieved chunks to the query.\n",
        "    * **Context recall**: Measure if all the relevant information from the source text is retrieved.\n",
        "\n",
        "**VI. Conclusion**\n",
        "\n",
        "Effective chunking is critical for building high-performing RAG systems. By understanding the different chunking strategies and their trade-offs, you can choose the best approach for your specific needs and create a RAG system that provides accurate, relevant, and contextually rich responses.  Remember to experiment and evaluate your choices to optimize performance.\n"
      ],
      "metadata": {
        "id": "9LZ_ESvU35Rh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jQXNvWV35xV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}